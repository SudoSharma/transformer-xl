{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import datetime\n",
    "import itertools\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "from collections import OrderedDict\n",
    "\n",
    "from fp16_opt import FP16_Module, FP16_Optimizer\n",
    "\n",
    "import numpy as np\n",
    "import pytz\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import tqdm\n",
    "from tensorboardX import SummaryWriter\n",
    "from torch.nn.parallel import DistributedDataParallel\n",
    "\n",
    "from data_utils import get_lm_corpus\n",
    "from mem_transformer import MemTransformerLM\n",
    "from lr_finder import LRFinder\n",
    "from pytorch_lamb import Lamb, log_lamb_rs\n",
    "\n",
    "from util import toscalar\n",
    "import util"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args: pass\n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defaults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.logdir = '/tmp/default'\n",
    "args.run_name = 'txl'\n",
    "args.data = '../data/wikitext-103'\n",
    "args.dataset = 'wt103'\n",
    "args.n_layer = 12\n",
    "args.n_head = 10\n",
    "args.d_head = 50\n",
    "args.d_embed = -1\n",
    "args.d_model = 500\n",
    "args.d_inner = 1000\n",
    "args.dropout = 0.0\n",
    "args.dropatt = 0.0\n",
    "args.init = 'normal'\n",
    "args.emb_init = 'normal'\n",
    "args.init_range = 0.1\n",
    "args.emb_init_range = 0.01\n",
    "args.init_std = 0.02\n",
    "args.proj_init_std = 0.01\n",
    "args.optim = 'adam'\n",
    "args.lr = 0.00025\n",
    "args.mom = 0.0\n",
    "args.wd = 0\n",
    "args.scheduler = 'cosine'\n",
    "args.warmup_tokens = 0\n",
    "args.decay_rate = 0.5\n",
    "args.lr_min = 0.0\n",
    "args.clip = 0.25\n",
    "args.clip_nonemb = False\n",
    "args.max_tokens = 1.8e9\n",
    "args.batch_size = 60\n",
    "args.tgt_len = 70\n",
    "args.eval_tgt_len = 50\n",
    "args.ext_len = 0\n",
    "args.mem_len = 0\n",
    "args.not_tied = False\n",
    "args.seed = 1111\n",
    "args.adaptive = False\n",
    "args.div_val = 1\n",
    "args.pre_lnorm = False\n",
    "args.log_interval = 200\n",
    "args.retune_interval = 5\n",
    "args.verbose_log_steps = 60\n",
    "args.eval_interval = 4000\n",
    "args.checkpoint_each_epoch = 0\n",
    "args.checkpoint = ''\n",
    "args.work_dir = None\n",
    "args.restart = False\n",
    "args.restart_dir = ''\n",
    "args.debug = False\n",
    "args.same_length = False\n",
    "args.attn_type = 0\n",
    "args.clamp_len = -1\n",
    "args.eta_min = 0.0\n",
    "args.gpu0_bsz = -1\n",
    "args.max_eval_steps = -1\n",
    "args.sample_softmax = -1\n",
    "args.patience = 0\n",
    "args.finetune_v2 = False\n",
    "args.finetune_v3 = False\n",
    "args.num_gpu = 1\n",
    "args.bpe = False\n",
    "args.fp16 = False\n",
    "args.static_loss_scale = 1.\n",
    "args.dynamic_loss_scale = False\n",
    "args.distributed = False\n",
    "args.dist_url = 'env://'\n",
    "args.dist_backend = 'nccl'\n",
    "args.local_rank = 0\n",
    "args.skip_auto_shutdown = False\n",
    "args.auto_shutdown_success_delay_mins = 10\n",
    "args.auto_shutdown_failure_delay_mins = 60\n",
    "args.tied = not args.not_tied"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.seed = 1111\n",
    "args.data = '../data/wikitext-2'\n",
    "args.dataset = 'wt2'\n",
    "args.adaptive = True\n",
    "args.log_interval = 100\n",
    "args.n_layer = 16\n",
    "args.d_model = 512\n",
    "args.n_head = 8\n",
    "args.d_head = 48\n",
    "args.d_inner = 2048\n",
    "args.dropout = 0.1\n",
    "args.optim = 'lamb'\n",
    "args.lr = 0.001\n",
    "args.wd = 0 \n",
    "args.max_tokens = 100000\n",
    "args.tgt_len = 128\n",
    "args.mem_len = 128\n",
    "args.eval_tgt_len = 128\n",
    "args.batch_size = 32\n",
    "args.eval_interval = 4000\n",
    "args.skip_auto_shutdown = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jupyter Notebook Distributed Training Hack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"RANK\"] = '0'\n",
    "os.environ[\"MASTER_ADDR\"] = '127.0.0.1'\n",
    "os.environ[\"MASTER_PORT\"] = '29500'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cached dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Torch version: 1.1.0\n",
      "====================================================================================================\n",
      "    - logdir : /tmp/default\n",
      "    - run_name : txl\n",
      "    - data : ../data/wikitext-2\n",
      "    - dataset : wt2\n",
      "    - n_layer : 16\n",
      "    - n_head : 8\n",
      "    - d_head : 48\n",
      "    - d_embed : 512\n",
      "    - d_model : 512\n",
      "    - d_inner : 2048\n",
      "    - dropout : 0.1\n",
      "    - dropatt : 0.0\n",
      "    - init : normal\n",
      "    - emb_init : normal\n",
      "    - init_range : 0.1\n",
      "    - emb_init_range : 0.01\n",
      "    - init_std : 0.02\n",
      "    - proj_init_std : 0.01\n",
      "    - optim : lamb\n",
      "    - lr : 0.001\n",
      "    - mom : 0.0\n",
      "    - wd : 0\n",
      "    - scheduler : cosine\n",
      "    - warmup_tokens : 0\n",
      "    - decay_rate : 0.5\n",
      "    - lr_min : 0.0\n",
      "    - clip : 0.25\n",
      "    - clip_nonemb : False\n",
      "    - max_tokens : 100000\n",
      "    - batch_size : 32\n",
      "    - tgt_len : 128\n",
      "    - eval_tgt_len : 128\n",
      "    - ext_len : 0\n",
      "    - mem_len : 128\n",
      "    - not_tied : False\n",
      "    - seed : 1111\n",
      "    - adaptive : True\n",
      "    - div_val : 1\n",
      "    - pre_lnorm : False\n",
      "    - log_interval : 100\n",
      "    - retune_interval : 5\n",
      "    - verbose_log_steps : 60\n",
      "    - eval_interval : 4000\n",
      "    - checkpoint_each_epoch : 0\n",
      "    - checkpoint : \n",
      "    - work_dir : /tmp/default\n",
      "    - restart : False\n",
      "    - restart_dir : \n",
      "    - debug : False\n",
      "    - same_length : False\n",
      "    - attn_type : 0\n",
      "    - clamp_len : -1\n",
      "    - eta_min : 0.0\n",
      "    - gpu0_bsz : -1\n",
      "    - max_eval_steps : -1\n",
      "    - sample_softmax : -1\n",
      "    - patience : 0\n",
      "    - finetune_v2 : False\n",
      "    - finetune_v3 : False\n",
      "    - num_gpu : 1\n",
      "    - bpe : False\n",
      "    - fp16 : False\n",
      "    - static_loss_scale : 1.0\n",
      "    - dynamic_loss_scale : False\n",
      "    - distributed : False\n",
      "    - dist_url : env://\n",
      "    - dist_backend : nccl\n",
      "    - local_rank : 0\n",
      "    - skip_auto_shutdown : True\n",
      "    - auto_shutdown_success_delay_mins : 10\n",
      "    - auto_shutdown_failure_delay_mins : 60\n",
      "    - tied : True\n",
      "    - n_token : 33278\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# global variables\n",
    "global_timeit_dict = OrderedDict()\n",
    "global_example_count = 0\n",
    "global_token_count = 0\n",
    "event_writer = util.NoOp()\n",
    "logdir = None\n",
    "epoch = 0\n",
    "train_step = 0\n",
    "optimizer = None\n",
    "scheduler = None\n",
    "\n",
    "local_rank = args.local_rank\n",
    "global_rank = util.get_global_rank()\n",
    "max_rank = util.get_world_size()\n",
    "torch.cuda.set_device(args.local_rank)\n",
    "\n",
    "# break into PDB debugger on exception\n",
    "if global_rank == 0:\n",
    "    util.pdb_on_error()\n",
    "\n",
    "\n",
    "class FileLogger:\n",
    "    def __init__(self, output_dir: str, is_master=False, is_rank0=False):\n",
    "        self.output_dir = output_dir\n",
    "        if not os.path.exists(self.output_dir):\n",
    "            if global_rank == 0:\n",
    "                os.makedirs(self.output_dir)\n",
    "        # only log on one process per node\n",
    "        if is_rank0:\n",
    "            self.logger = FileLogger.get_logger(output_dir, log_to_file=is_master)\n",
    "        else:\n",
    "            self.logger = util.NoOp()\n",
    "\n",
    "    def exception(self, *args_, **kwargs):\n",
    "        return self.logger.exception(*args_, **kwargs)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_logger(output_dir: str, log_to_file: bool = True):\n",
    "        logger_ = logging.getLogger('txl training')\n",
    "        logger_.setLevel(logging.DEBUG)\n",
    "        formatter = logging.Formatter('%(message)s')\n",
    "\n",
    "        if log_to_file:\n",
    "            vlog = logging.FileHandler(output_dir + '/info.log')\n",
    "            vlog.setLevel(logging.INFO)\n",
    "            vlog.setFormatter(formatter)\n",
    "            logger_.addHandler(vlog)\n",
    "\n",
    "            eventlog = logging.FileHandler(output_dir + '/warn.log')\n",
    "            eventlog.setLevel(logging.WARN)\n",
    "            eventlog.setFormatter(formatter)\n",
    "            logger_.addHandler(eventlog)\n",
    "\n",
    "            time_formatter = logging.Formatter('%(asctime)s - %(filename)s:%(lineno)d - %(message)s')\n",
    "            debuglog = logging.FileHandler(output_dir + '/debug.log')\n",
    "            debuglog.setLevel(logging.DEBUG)\n",
    "            debuglog.setFormatter(time_formatter)\n",
    "            logger_.addHandler(debuglog)\n",
    "\n",
    "        console = logging.StreamHandler()\n",
    "        console.setFormatter(formatter)\n",
    "        console.setLevel(logging.DEBUG)\n",
    "        logger_.addHandler(console)\n",
    "        return logger_\n",
    "\n",
    "    def debug(self, *args_):\n",
    "        self.logger.debug(*args_)\n",
    "\n",
    "    def warn(self, *args_):\n",
    "        self.logger.warn(*args_)\n",
    "\n",
    "    def info(self, *args_):\n",
    "        self.logger.info(*args_)\n",
    "\n",
    "\n",
    "class timeit:\n",
    "    \"\"\"Decorator to measure length of time spent in the block in millis and log\n",
    "  it to TensorBoard.\"\"\"\n",
    "\n",
    "    def __init__(self, tag=\"\", noop=False):\n",
    "        self.tag = tag\n",
    "        self.noop = noop\n",
    "\n",
    "    def __enter__(self):\n",
    "        if self.noop:\n",
    "            return self\n",
    "        self.start = time.perf_counter()\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, *_args):\n",
    "        if self.noop:\n",
    "            return\n",
    "        self.end = time.perf_counter()\n",
    "        interval_ms = 1000 * (self.end - self.start)\n",
    "        global_timeit_dict.setdefault(self.tag, []).append(interval_ms)\n",
    "        newtag = 'times/' + self.tag\n",
    "        log_tb(newtag, interval_ms)\n",
    "\n",
    "\n",
    "def log_tb(tag, val):\n",
    "    \"\"\"Log value to tensorboard (relies on global_example_count rather than step count to give comparable graphs across\n",
    "    batch sizes)\"\"\"\n",
    "    global global_token_count, event_writer\n",
    "    event_writer.add_scalar(tag, val, global_token_count)\n",
    "\n",
    "\n",
    "PT_TZ = pytz.timezone('America/Los_Angeles')\n",
    "\n",
    "\n",
    "def current_timestamp() -> str:\n",
    "    # timestamp format like 2019-04-15_11-29-51\n",
    "    # correct to local timezone (PDT) if running on AWS (which is UTC)\n",
    "    localtime = pytz.utc.localize(datetime.datetime.now(), is_dst=None).astimezone(PT_TZ)\n",
    "    return localtime.strftime('%Y-%m-%d_%H-%M-%S')\n",
    "\n",
    "\n",
    "if args.d_embed < 0:\n",
    "    args.d_embed = args.d_model\n",
    "\n",
    "assert args.ext_len >= 0, 'extended context length must be non-negative'\n",
    "\n",
    "if not args.work_dir:\n",
    "    args.work_dir = args.logdir\n",
    "\n",
    "logger = FileLogger(args.logdir, is_master=(global_rank == 0), is_rank0=(args.local_rank == 0))\n",
    "\n",
    "# Set the random seed manually for reproducibility.\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(args.seed)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "###############################################################################\n",
    "# Load data\n",
    "###############################################################################\n",
    "corpus = get_lm_corpus(args.data, args.dataset, use_bpe=args.bpe)\n",
    "ntokens = len(corpus.vocab)\n",
    "args.n_token = ntokens\n",
    "\n",
    "eval_batch_size = args.batch_size * 2\n",
    "tr_iter, va_iter, te_iter = [\n",
    "    corpus.get_dist_iterator(\n",
    "        split, global_rank, max_rank, args.batch_size, args.tgt_len,\n",
    "        device=device, ext_len=args.ext_len)\n",
    "    for split in ('train', 'valid', 'test')\n",
    "]\n",
    "\n",
    "# adaptive softmax / embedding\n",
    "cutoffs, tie_projs = [], [False]\n",
    "if args.adaptive:\n",
    "    assert args.dataset in ['wt103', 'lm1b', 'wt2']\n",
    "    if args.dataset == 'wt103' or args.dataset == 'wt2':\n",
    "        if args.bpe:\n",
    "            cutoffs = [5000, 10000, 40000]\n",
    "        else:\n",
    "            cutoffs = [20000, 40000, 200000]\n",
    "        tie_projs += [True] * len(cutoffs)\n",
    "    elif args.dataset == 'lm1b':\n",
    "        cutoffs = [60000, 100000, 640000]\n",
    "        tie_projs += [False] * len(cutoffs)\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# Build the model\n",
    "###############################################################################\n",
    "def init_weight(weight):\n",
    "    if args.init == 'uniform':\n",
    "        nn.init.uniform_(weight, -args.init_range, args.init_range)\n",
    "    elif args.init == 'normal':\n",
    "        nn.init.normal_(weight, 0.0, args.init_std)\n",
    "\n",
    "\n",
    "def init_bias(bias):\n",
    "    nn.init.constant_(bias, 0.0)\n",
    "\n",
    "\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Linear') != -1:\n",
    "        if hasattr(m, 'weight') and m.weight is not None:\n",
    "            init_weight(m.weight)\n",
    "        if hasattr(m, 'bias') and m.bias is not None:\n",
    "            init_bias(m.bias)\n",
    "    elif classname.find('AdaptiveEmbedding') != -1:\n",
    "        if hasattr(m, 'emb_projs'):\n",
    "            for i in range(len(m.emb_projs)):\n",
    "                if m.emb_projs[i] is not None:\n",
    "                    nn.init.normal_(m.emb_projs[i], 0.0, args.proj_init_std)\n",
    "    elif classname.find('Embedding') != -1:\n",
    "        if hasattr(m, 'weight'):\n",
    "            init_weight(m.weight)\n",
    "    elif classname.find('ProjectedAdaptiveLogSoftmax') != -1:\n",
    "        if hasattr(m, 'cluster_weight') and m.cluster_weight is not None:\n",
    "            init_weight(m.cluster_weight)\n",
    "        if hasattr(m, 'cluster_bias') and m.cluster_bias is not None:\n",
    "            init_bias(m.cluster_bias)\n",
    "        if hasattr(m, 'out_projs'):\n",
    "            for i in range(len(m.out_projs)):\n",
    "                if m.out_projs[i] is not None:\n",
    "                    nn.init.normal_(m.out_projs[i], 0.0, args.proj_init_std)\n",
    "    elif classname.find('LayerNorm') != -1:\n",
    "        if hasattr(m, 'weight'):\n",
    "            nn.init.normal_(m.weight, 1.0, args.init_std)\n",
    "        if hasattr(m, 'bias') and m.bias is not None:\n",
    "            init_bias(m.bias)\n",
    "    elif classname.find('TransformerLM') != -1:\n",
    "        if hasattr(m, 'r_emb'):\n",
    "            init_weight(m.r_emb)\n",
    "        if hasattr(m, 'r_w_bias'):\n",
    "            init_weight(m.r_w_bias)\n",
    "        if hasattr(m, 'r_r_bias'):\n",
    "            init_weight(m.r_r_bias)\n",
    "        if hasattr(m, 'r_bias'):\n",
    "            init_bias(m.r_bias)\n",
    "\n",
    "\n",
    "# todo(y): move into main()\n",
    "logger.info(\"Torch version: {}\".format(torch.__version__))\n",
    "logger.info('=' * 100)\n",
    "for k, v in args.__dict__.items():\n",
    "    logger.info('    - {} : {}'.format(k, v))\n",
    "logger.info('=' * 100)\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# Training code\n",
    "###############################################################################\n",
    "\n",
    "\n",
    "def evaluate(eval_iter):\n",
    "    # Turn on evaluation mode which disables dropout.\n",
    "    model.eval()\n",
    "\n",
    "    # Have to unwrap twice: DDP & FP16\n",
    "    model_to_reset = model.module.module if args.fp16 else model.module\n",
    "    # If the model does not use memory at all, make the ext_len longer.\n",
    "    # Otherwise, make the mem_len longer and keep the ext_len the same.\n",
    "    if args.mem_len == 0:\n",
    "        model_to_reset.reset_length(\n",
    "            args.eval_tgt_len, args.ext_len + args.tgt_len - args.eval_tgt_len, args.mem_len)\n",
    "    else:\n",
    "        model_to_reset.reset_length(\n",
    "            args.eval_tgt_len, args.ext_len, args.mem_len + args.tgt_len - args.eval_tgt_len)\n",
    "  \n",
    "    # Evaluation\n",
    "    total_len, total_loss = 0, 0.\n",
    "    with torch.no_grad():\n",
    "        mems = tuple()\n",
    "        bar = tqdm.tqdm(eval_iter, leave=False, desc=\"Eval\")\n",
    "        for i, (data, target, seq_len) in enumerate(bar):\n",
    "            if args.max_eval_steps > 0:\n",
    "                if i >= args.max_eval_steps:\n",
    "                    break\n",
    "            ret = model(data, target, *mems)\n",
    "            loss, mems = ret[0], ret[1:]\n",
    "            loss = loss.mean()\n",
    "            bar.set_description(f'Eval loss {loss:.2f}')\n",
    "            total_loss += seq_len * loss.float().item()\n",
    "            total_len += seq_len\n",
    "\n",
    "    # Switch back to the training mode\n",
    "    model_to_reset.reset_length(args.tgt_len, args.ext_len, args.mem_len)\n",
    "    model.train()\n",
    "\n",
    "    return total_loss / total_len\n",
    "\n",
    "\n",
    "def train():\n",
    "    global global_example_count, global_token_count, event_writer, logdir, train_loss, best_val_loss, \\\n",
    "        train_step, last_log_step, epoch, optimizer, scheduler\n",
    "    # Turn on training mode which enables dropout.\n",
    "    model.train()\n",
    "\n",
    "    log_tb('sizes/batch_size', args.batch_size)\n",
    "    log_tb('sizes/seq_size', args.tgt_len)\n",
    "\n",
    "    mems = tuple()\n",
    "    train_iter = tr_iter\n",
    "    log_start_time = time.time()\n",
    "    for batch, (data, target, seq_len) in enumerate(train_iter):\n",
    "        # TODO(y): batch is dimension 1, why?\n",
    "\n",
    "        assert seq_len == data.shape[0]\n",
    "        for i in range(1, data.shape[0]):\n",
    "            assert torch.all(torch.eq(data[i], target[i - 1]))\n",
    "            break\n",
    "\n",
    "        batch_total = torch.tensor(data.shape[1]).to(device)\n",
    "        batch_total = batch_total.to(device)  # needed for NCCL sync\n",
    "        batch_total = util.dist_sum_tensor(batch_total)  # global batch size\n",
    "\n",
    "        total_tokens = batch_total.item() * seq_len\n",
    "        should_log = train_step < args.verbose_log_steps or train_step % args.log_interval == 0\n",
    "\n",
    "        global_token_count += total_tokens\n",
    "        model.zero_grad()\n",
    "        ret = model(data, target, *mems)\n",
    "        loss, mems = ret[0], ret[1:]\n",
    "        loss = loss.float().mean().type_as(loss)\n",
    "        with timeit('backwards', noop=not should_log):\n",
    "            if args.fp16:\n",
    "                optimizer.backward(loss)\n",
    "            else:\n",
    "                loss.backward()\n",
    "        train_loss += loss.float().item()\n",
    "\n",
    "        # sparse gradient update:\n",
    "        # basically never zero gradients\n",
    "        # after first backward step, you mask under 0.9 (for ex.)\n",
    "        # do optimizer.step()\n",
    "        # now do opposite mask on original grads (mask all grads over 0.9)\n",
    "        # these are actual gradients that get added on to next update\n",
    "        # do backward again and start over\n",
    "        # for individual params the sparse update interval may be different\n",
    "        # but should cancel out by learning rate scaling \n",
    "\n",
    "        if args.fp16:\n",
    "            optimizer.clip_master_grads(args.clip)\n",
    "        else:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), args.clip)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        # step-wise learning rate annealing\n",
    "        train_step += 1\n",
    "\n",
    "        if not (args.fp16 and optimizer.overflow):\n",
    "            if args.scheduler in ['cosine', 'constant', 'dev_perf']:\n",
    "                # linear warmup stage\n",
    "                if global_token_count < args.warmup_tokens:\n",
    "                    curr_lr = args.lr * global_token_count / args.warmup_tokens\n",
    "                    optimizer.param_groups[0]['lr'] = curr_lr\n",
    "                else:\n",
    "                    if args.scheduler == 'cosine':\n",
    "                        scheduler.step(global_token_count)\n",
    "            else:\n",
    "                scheduler.step(global_token_count)\n",
    "        else:\n",
    "            print(\"skipped iteration!\")\n",
    "\n",
    "        if should_log:\n",
    "            elapsed_time = time.time() - log_start_time\n",
    "            elapsed_steps = train_step - last_log_step\n",
    "\n",
    "            # compute average loss over last logging interval\n",
    "            cur_loss = train_loss / elapsed_steps\n",
    "            log_str = '| epoch {:3d} step {:>8d} | {:>6d} batches | lr {:.3g} ' \\\n",
    "                      '| ms/batch {:5.2f} | loss {:5.2f}'.format(epoch, train_step, batch + 1,\n",
    "                                                                 optimizer.param_groups[0]['lr'],\n",
    "                                                                 elapsed_time * 1000 / elapsed_steps, cur_loss)\n",
    "            if args.dataset in ['enwik8', 'text8']:\n",
    "                log_str += ' | bpc {:9.5f}'.format(cur_loss / math.log(2))\n",
    "            else:\n",
    "                log_str += ' | ppl {:9.3f}'.format(math.exp(cur_loss))\n",
    "            logger.info(log_str)\n",
    "            log_tb('loss/epoch', epoch)\n",
    "            log_tb('loss/loss', cur_loss)\n",
    "            log_tb('loss/ppl', math.exp(cur_loss))\n",
    "            log_tb('times/step', 1000 * elapsed_time / elapsed_steps)\n",
    "            current_lr = optimizer.param_groups[0]['lr']\n",
    "            log_tb('lr', current_lr)\n",
    "            log_tb('lr_normalized1', current_lr / toscalar(batch_total))\n",
    "            log_tb('lr_normalized2', current_lr / toscalar(total_tokens))\n",
    "            if args.optim == 'lamb':\n",
    "                log_lamb_rs(optimizer, event_writer, global_token_count)\n",
    "\n",
    "            time_per_batch = elapsed_time / elapsed_steps\n",
    "            time_per_sample = time_per_batch / args.batch_size\n",
    "            time_per_token = time_per_sample / args.tgt_len\n",
    "\n",
    "            log_tb('times/batches_per_sec', 1 / time_per_batch)\n",
    "            log_tb('times/samples_per_sec', 1 / time_per_sample)\n",
    "            log_tb('times/tokens_per_sec', 1 / time_per_token)\n",
    "\n",
    "            if str(device) == 'cuda':\n",
    "                log_tb(\"memory/allocated_gb\", torch.cuda.memory_allocated() / 1e9)\n",
    "                log_tb(\"memory/max_allocated_gb\", torch.cuda.max_memory_allocated() / 1e9)\n",
    "                log_tb(\"memory/cached_gb\", torch.cuda.memory_cached() / 1e9)\n",
    "                log_tb(\"memory/max_cached_gb\", torch.cuda.max_memory_cached() / 1e9)\n",
    "\n",
    "            # todo(y): refactor to init loss at the top\n",
    "            train_loss = 0\n",
    "            log_start_time = time.time()\n",
    "            last_log_step = train_step\n",
    "\n",
    "        if train_step % args.eval_interval == 0:\n",
    "            eval_start_time = time.time()\n",
    "            val_loss = evaluate(va_iter)\n",
    "            if not best_val_loss or val_loss < best_val_loss:\n",
    "                if not args.debug:\n",
    "                    logger.info('Saving checkpoint for new best loss')\n",
    "                    util.dist_save_checkpoint(model, optimizer, args.logdir, suffix='best')\n",
    "\n",
    "                best_val_loss = val_loss\n",
    "\n",
    "            logger.info('-' * 100)\n",
    "            log_str = '| Eval {:3d} at step {:>8d} | time: {:5.2f}s ' \\\n",
    "                      '| valid loss {:5.2f}'.format(train_step // args.eval_interval, train_step, (time.time() -\n",
    "                                                                                                   eval_start_time),\n",
    "                                                    val_loss)\n",
    "            if args.dataset in ['enwik8', 'text8']:\n",
    "                log_str += ' | bpc {:9.5f}'.format(val_loss / math.log(2))\n",
    "            else:\n",
    "                log_str += ' | valid ppl {:9.3f}'.format(math.exp(val_loss))\n",
    "            logger.info(log_str)\n",
    "            logger.info('-' * 100)\n",
    "            log_tb('loss/val_loss', val_loss)\n",
    "            log_tb('loss/val_ppl', math.exp(val_loss))\n",
    "\n",
    "\n",
    "        # TODO: instead of stopping training, transition to constant small LR forever\n",
    "        if global_token_count >= args.max_tokens:\n",
    "            logger.info('-' * 100)\n",
    "            logger.info('End of training')\n",
    "            raise StopIteration()\n",
    "\n",
    "    if args.checkpoint_each_epoch:\n",
    "        logger.info(f'Saving checkpoint for epoch {epoch}')\n",
    "        util.dist_save_checkpoint(model, optimizer, args.logdir, suffix=f'{epoch}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Distributed initializing process group with nccl, env://, 1\n",
      "Distributed: success (0/1)\n",
      "params 66430721 non_emb_params 49356800\n",
      "| epoch   1 step        1 |      1 batches | lr 0.001 | ms/batch 575.21 | loss 10.20 | ppl 26780.744\n",
      "| epoch   1 step        2 |      2 batches | lr 0.001 | ms/batch 352.87 | loss 10.07 | ppl 23701.479\n",
      "| epoch   1 step        3 |      3 batches | lr 0.001 | ms/batch 351.70 | loss 10.02 | ppl 22532.901\n",
      "| epoch   1 step        4 |      4 batches | lr 0.001 | ms/batch 351.48 | loss 10.02 | ppl 22426.759\n",
      "| epoch   1 step        5 |      5 batches | lr 0.001 | ms/batch 353.61 | loss  9.94 | ppl 20683.861\n",
      "| epoch   1 step        6 |      6 batches | lr 0.001 | ms/batch 356.37 | loss  9.98 | ppl 21691.223\n",
      "| epoch   1 step        7 |      7 batches | lr 0.001 | ms/batch 354.87 | loss  9.86 | ppl 19193.146\n",
      "| epoch   1 step        8 |      8 batches | lr 0.001 | ms/batch 352.47 | loss  9.88 | ppl 19510.404\n",
      "| epoch   1 step        9 |      9 batches | lr 0.001 | ms/batch 352.84 | loss  9.84 | ppl 18766.157\n",
      "| epoch   1 step       10 |     10 batches | lr 0.001 | ms/batch 352.84 | loss  9.83 | ppl 18595.646\n",
      "| epoch   1 step       11 |     11 batches | lr 0.001 | ms/batch 353.95 | loss  9.76 | ppl 17386.274\n",
      "| epoch   1 step       12 |     12 batches | lr 0.001 | ms/batch 354.59 | loss  9.74 | ppl 17054.367\n",
      "| epoch   1 step       13 |     13 batches | lr 0.001 | ms/batch 353.82 | loss  9.72 | ppl 16626.227\n",
      "| epoch   1 step       14 |     14 batches | lr 0.001 | ms/batch 354.35 | loss  9.72 | ppl 16618.982\n",
      "| epoch   1 step       15 |     15 batches | lr 0.001 | ms/batch 352.61 | loss  9.66 | ppl 15651.012\n",
      "| epoch   1 step       16 |     16 batches | lr 0.001 | ms/batch 352.21 | loss  9.60 | ppl 14768.251\n",
      "| epoch   1 step       17 |     17 batches | lr 0.001 | ms/batch 352.18 | loss  9.58 | ppl 14523.783\n",
      "| epoch   1 step       18 |     18 batches | lr 0.001 | ms/batch 353.43 | loss  9.63 | ppl 15276.957\n",
      "| epoch   1 step       19 |     19 batches | lr 0.000999 | ms/batch 356.42 | loss  9.58 | ppl 14415.020\n",
      "| epoch   1 step       20 |     20 batches | lr 0.000999 | ms/batch 357.02 | loss  9.57 | ppl 14384.848\n",
      "| epoch   1 step       21 |     21 batches | lr 0.000999 | ms/batch 353.64 | loss  9.52 | ppl 13672.736\n",
      "| epoch   1 step       22 |     22 batches | lr 0.000999 | ms/batch 352.69 | loss  9.53 | ppl 13717.468\n",
      "| epoch   1 step       23 |     23 batches | lr 0.000999 | ms/batch 353.44 | loss  9.49 | ppl 13247.925\n",
      "| epoch   1 step       24 |     24 batches | lr 0.000997 | ms/batch 353.67 | loss  9.47 | ppl 12905.518\n",
      "| epoch   1 step       25 |     25 batches | lr 0.000998 | ms/batch 352.55 | loss  9.43 | ppl 12512.967\n",
      "----------------------------------------------------------------------------------------------------\n",
      "End of training\n",
      "Loading best checkpoint\n",
      "/home/sudosharma/miniconda3/envs/dgc/lib/python3.6/site-packages/ipykernel_launcher.py:70: DeprecationWarning: The 'warn' method is deprecated, use 'warning' instead\n",
      "no model file, using current model for loss\n",
      "====================================================================================================\n",
      "| End of training | test loss  9.18 | test ppl  9722.815\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    global global_example_count, global_token_count, event_writer, logdir, train_step, train_loss, last_log_step, \\\n",
    "        best_val_loss, epoch, model, optimizer, scheduler\n",
    "\n",
    "    if args.local_rank > 0:\n",
    "        pass  # skip shutdown when rank is explicitly set + not zero rank\n",
    "    else:\n",
    "        os.system('shutdown -c')\n",
    "\n",
    "    logger.info(\n",
    "        f'Distributed initializing process group with {args.dist_backend}, {args.dist_url}, {util.get_world_size()}')\n",
    "\n",
    "    dist.init_process_group(backend=args.dist_backend,\n",
    "                            init_method=args.dist_url,\n",
    "                            world_size=util.get_world_size())\n",
    "    assert (util.get_world_size() == dist.get_world_size())\n",
    "    logger.info(\"Distributed: success (%d/%d)\" % (args.local_rank, dist.get_world_size()))\n",
    "\n",
    "    model = MemTransformerLM(ntokens, args.n_layer, args.n_head, args.d_model,\n",
    "                             args.d_head, args.d_inner, args.dropout, args.dropatt,\n",
    "                             tie_weight=args.tied, d_embed=args.d_embed, div_val=args.div_val,\n",
    "                             tie_projs=tie_projs, pre_lnorm=args.pre_lnorm, tgt_len=args.tgt_len,\n",
    "                             ext_len=args.ext_len, mem_len=args.mem_len, cutoffs=cutoffs,\n",
    "                             same_length=args.same_length, attn_type=args.attn_type,\n",
    "                             clamp_len=args.clamp_len, sample_softmax=args.sample_softmax)\n",
    "\n",
    "    # log model info\n",
    "    n_all_param = sum([p.nelement() for p in model.parameters()])\n",
    "    log_tb('sizes/params', n_all_param)\n",
    "    n_nonemb_param = sum([p.nelement() for p in model.layers.parameters()])\n",
    "    log_tb('sizes/non_emb_params', n_nonemb_param)\n",
    "    logger.info('params %s non_emb_params %s', n_all_param, n_nonemb_param)\n",
    "    # optimizer\n",
    "    if args.optim.lower() == 'sgd':\n",
    "        optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=args.mom)\n",
    "    elif args.optim.lower() == 'lamb':\n",
    "        optimizer = Lamb(model.parameters(), lr=args.lr, weight_decay=args.wd)\n",
    "    else:\n",
    "        assert args.optim.lower() == 'adam'\n",
    "        # TODO(b): try , betas=(0.9, 0.99))\n",
    "        optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.wd)\n",
    "\n",
    "    # scheduler\n",
    "    if args.scheduler == 'cosine':\n",
    "        # here we do not set eta_min to lr_min to be backward compatible\n",
    "        # because in previous versions eta_min is default to 0\n",
    "        # rather than the default value of lr_min 1e-6\n",
    "        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, args.max_tokens, eta_min=args.eta_min)\n",
    "    elif args.scheduler == 'finder':\n",
    "        scheduler = LRFinder(optimizer, args.max_tokens, init_value=args.lr / 1e3)\n",
    "    elif args.scheduler == 'constant':\n",
    "        pass\n",
    "\n",
    "    # todo(y): only if rank0\n",
    "    model.apply(weights_init)\n",
    "    model.word_emb.apply(weights_init)  # ensure embedding init is not overridden by out_layer in case of weight sharing\n",
    "\n",
    "\n",
    "    if args.checkpoint:\n",
    "        #        util.dist_restore_from_checkpoint(ddp_model=model, checkpoint_fn=args.checkpoint)\n",
    "        if global_rank == 0:\n",
    "            util.restore_from_checkpoint(model=model, checkpoint_fn=args.checkpoint)\n",
    "\n",
    "        # with open(os.path.join(args.restart_dir, 'optimizer.pt'), 'rb') as f:\n",
    "        #     opt_state_dict = torch.load(f)\n",
    "        #     optimizer.load_state_dict(opt_state_dict)\n",
    "\n",
    "    \n",
    "    if args.fp16:\n",
    "        # If args.dynamic_loss_scale is False, static_loss_scale will be used.\n",
    "        model = FP16_Module(model)\n",
    "\n",
    "    model = model.to(device)\n",
    "    if args.fp16:\n",
    "        # If args.dynamic_loss_scale is True, it will take precedence over static_loss_scale.\n",
    "        optimizer = FP16_Optimizer(optimizer,\n",
    "                                   static_loss_scale=args.static_loss_scale,\n",
    "                                   dynamic_loss_scale=args.dynamic_loss_scale,\n",
    "                                   dynamic_loss_args={'init_scale': 2 ** 16},\n",
    "                                   verbose=True)\n",
    "    model = model.to(device)\n",
    "\n",
    "    #    model = util.DistributedDataParallel(model, device_ids=[args.local_rank], output_device=args.local_rank)\n",
    "    model = DistributedDataParallel(model, device_ids=[args.local_rank], output_device=args.local_rank)\n",
    "\n",
    "                                          \n",
    "    logdir = args.logdir\n",
    "    assert os.path.exists(logdir)\n",
    "\n",
    "    if global_rank == 0 and not args.debug:\n",
    "        event_writer = SummaryWriter(logdir)\n",
    "\n",
    "    log_tb(\"first\", time.time())\n",
    "    event_writer.add_text('args', str(args))\n",
    "\n",
    "    # test checkpoint writing\n",
    "    if args.checkpoint_each_epoch:\n",
    "        logger.info(f'Saving checkpoint for epoch {epoch}')\n",
    "        util.dist_save_checkpoint(model, optimizer, args.logdir, suffix=f'{0}')\n",
    "\n",
    "    # Loop over epochs.\n",
    "    train_step = 0\n",
    "    train_loss = 0\n",
    "    last_log_step = 0\n",
    "    best_val_loss = None\n",
    "\n",
    "    # At any point you can hit Ctrl + C to break out of training early.\n",
    "    try:\n",
    "        for epoch in itertools.count(start=1):\n",
    "            train()\n",
    "    except KeyboardInterrupt:\n",
    "        logger.info('-' * 100)\n",
    "        logger.info('Exiting from training early')\n",
    "    except StopIteration:\n",
    "        pass\n",
    "\n",
    "    # Load the best saved model.\n",
    "    logger.info(\"Loading best checkpoint\")\n",
    "    model_file = os.path.join(args.work_dir, 'model-best.pt')\n",
    "    if os.path.exists(model_file):\n",
    "        with open(model_file, 'rb') as model_f:\n",
    "            with timeit('load'):\n",
    "                model = torch.load(f, map_location = lambda storage,\n",
    "                                loc: storage.cuda(args.local_rank))\n",
    "                if args.distributed:\n",
    "                    model = DistributedDataParallel(\n",
    "                        model,\n",
    "                        device_ids=[args.local_rank],\n",
    "                        output_device=args.local_rank)\n",
    "    else:\n",
    "        logger.warn('no model file, using current model for loss')\n",
    "\n",
    "    # Run on test data.\n",
    "    test_loss = evaluate(te_iter)\n",
    "    logger.info('=' * 100)\n",
    "    if args.dataset in ['enwik8', 'text8']:\n",
    "        logger.info('| End of training | test loss {:5.2f} | test bpc {:9.5f}'.format(\n",
    "            test_loss, test_loss / math.log(2)))\n",
    "    else:\n",
    "        logger.info('| End of training | test loss {:5.2f} | test ppl {:9.3f}'.format(\n",
    "            test_loss, math.exp(test_loss)))\n",
    "    log_tb('loss/test_loss', test_loss)\n",
    "    log_tb('loss/test_ppl', math.exp(test_loss))\n",
    "\n",
    "    logger.info('=' * 100)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    try:\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\", category=UserWarning)\n",
    "            main()\n",
    "        if not args.skip_auto_shutdown and args.local_rank == 0:\n",
    "            os.system(f'sudo shutdown -h -P +{args.auto_shutdown_success_delay_mins}')\n",
    "    except Exception as e:\n",
    "        import traceback\n",
    "\n",
    "        traceback.print_exc(file=sys.stdout)\n",
    "        # Logger automatically picks up exc info from context.\n",
    "        logger.exception('Failed')\n",
    "        # in case of exception, wait 2 hours before shutting down\n",
    "        if not args.skip_auto_shutdown:\n",
    "            os.system(f'sudo shutdown -h -P +{args.auto_shutdown_failure_delay_mins}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
